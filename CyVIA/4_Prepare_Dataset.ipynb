{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** CyVID Dataset Preparation v5 ***\n",
      "\n",
      "Start Time: 14:10:49\n",
      "|Directory analysis already exists.\n",
      "Removing old files... Done.\n",
      "\n",
      "Reading, summarizing, and writing NVD JSON Data...\n",
      "Server status: (3.1.1) up and running!\n",
      "\b|Insertion complete.\n",
      "\n",
      "Total number of vulnerability records found: 166759 \n",
      "Vulnerabilities with missing information:\n",
      "Lang:\t\t9783\n",
      "\n",
      "CWE-ID:\t\t23873\n",
      "NVD-CWE-Other\t26646\n",
      "NVD-CWE-noinfo:\t15308\n",
      "Total CWE:\t65827\n",
      "Mitigation:\t41954\n",
      "Consequences:\t41954\n",
      "\n",
      "Severity:\t9829\n",
      "CVSS_V2:\t9829\n",
      "CVSS_V3:\t83171\n",
      "Vul_access:\t9829\n",
      "UserIntReq:\t10854\n",
      "PublishDate:\t0\n",
      "ModifiedDate:\t0\n",
      "UnicodeError:\t0\n",
      "\n",
      "CWE_Desc:\t41954\n",
      "CWE_Plat:\t41954\n",
      "CWE_Af_Res:\t41954\n",
      "M_OS:\t\t0\n",
      "M_SW:\t\t0\n",
      "M_Ports:\t0\n",
      "\n",
      "\n",
      "\n",
      "Data collected.\n",
      "File(s) saved at analysis/. and Database: cyvia_dataset created.\n",
      "Execution time: 0:21:04.759101\n",
      "\b"
     ]
    }
   ],
   "source": [
    "# NVD and MITRE Feed Handler...\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en_core_web_lg\n",
    "\n",
    "# Imports\n",
    "import json, csv, os, requests, re, zipfile, spacy, timeit\n",
    "import pandas as pd  \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from spacy.matcher import Matcher\n",
    "from datetime import datetime\n",
    "\n",
    "# global variables and other classes\n",
    "json_files_path, analysis_path, summary_csv, db_name = 'nvd_json_files', 'analysis', 'cyvia_dataset.csv', 'cyvia_dataset'\n",
    "mitre_csv_path = 'mitre_csv_files/cwe_combined.csv'\n",
    "cwe_data = pd.read_csv(mitre_csv_path) # load cwe data file in dataframs\n",
    "summarized_dataset = analysis_path + '/' + summary_csv\n",
    "enc = 'utf-8' # enc = 'utf-16', enc = 'iso-8859-15', enc = 'cp437'\n",
    "\n",
    "import functions as fn # functions class\n",
    "func = fn.functions()\n",
    "import Spinner as sp # spinner while performing db operations\n",
    "spinner = sp.Spinner()\n",
    "\n",
    "# Prepare data from downloaded JSON files...\n",
    "# This step reads the JSON files and writes the collected data to a CSV file and database. \n",
    "def read_and_summarize():\n",
    "    print('Reading, summarizing, and writing NVD JSON Data...')\n",
    "    column_names = ['_id','lang', 'cwe_id', 'cwe_desc', 'cwe_plat', 'cwe_af_res', 'cwe_consequences', 'cwe_mitigations', 'severity', 'cvss_v2', 'cvss_v3', 'vul_access_vector', 'user_int_req', 'os', 'sw', 'ports', 'published_date','modified_date','description', 'url_and_tags']\n",
    "\n",
    "    # connect database\n",
    "    func.connect_db(db_name, True) # create a new database\n",
    "    # Start writing data...    \n",
    "    with open(summarized_dataset, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # write the header...\n",
    "        writer.writerow(column_names)\n",
    "        # data field variables as above line\n",
    "        CVE_ID, CVE_Lang, CWE_ID, CWE_Desc, CWE_Plat, CWE_Af_Res, CVE_Sev, CVSS_V2, CVSS_V3, CVE_VAV, CVE_UIR, CVE_PD, CVE_MD, CVE_Desc = \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "        CWE_Consequences, CWE_Mitigations = {}, {} # Dictionaries\n",
    "        CVE_OS, CVE_SW, CVE_Ports = [], [], [] # Lists\n",
    "        # missing data field counters\n",
    "        M_CVE_Lang, M_CWE_ID, M_CWE_ID_Other, M_CWE_ID_Other2, M_CWE_Desc, M_CWE_Plat, M_CWE_Af_Res, M_CVE_Sev, M_CVSS_V2, M_CVSS_V3, M_CVE_VAV, M_CVE_UIR, M_OS, M_SW, M_Ports, M_CVE_PD, M_CVE_MD, unicode_error, M_CWE_Mitigations, M_CWE_Consequences = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "        vul_info_counter=0 # total number of handled vulnerabilities\n",
    "        \n",
    "        #Loop through input files in the directory...\n",
    "        for root, dirs, files in os.walk(json_files_path + '/', topdown=False):\n",
    "            for name in files:\n",
    "                with open(os.path.join(root, name), 'r', encoding=enc) as f:\n",
    "                    data=json.load(f)\n",
    "\n",
    "                #Write json file data as rows to the new file...\n",
    "                for i in range(0, len(data['CVE_Items'])):\n",
    "                    vul_info_counter+=1 # increment vul counter\n",
    "                    # extract URL and Tag references \n",
    "                    url_tag_dict = {}\n",
    "                    # collecting references, can be more than one.\n",
    "                    for x in range(0, len(data['CVE_Items'][i]['cve']['references']['reference_data'])): # first item till the last item.\n",
    "                        url_tag_dict['URL'+str(x)] = data['CVE_Items'][i]['cve']['references']['reference_data'][x]['url'] # append dict.\n",
    "                        # for tag list, add more than one tags.\n",
    "                        for y in range(0, len(data['CVE_Items'][i]['cve']['references']['reference_data'][x]['tags'])): \n",
    "                            url_tag_dict['URL'+str(x)+'-Tag'+str(y)] = data['CVE_Items'][i]['cve']['references']['reference_data'][x]['tags'][y]             \n",
    "\n",
    "                    # Fetching field[i]values from JSON file\n",
    "                    # Field: CVE_ID, CVE-Description, available for all entries.\n",
    "                    CVE_ID = data['CVE_Items'][i]['cve']['CVE_data_meta']['ID']\n",
    "                    CVE_Desc = data['CVE_Items'][i]['cve']['description']['description_data'][0]['value']\n",
    "                    # Extract keywords from the above description\n",
    "                    CVE_OS, CVE_SW, CVE_Ports = [], [], []\n",
    "                    #CVE_OS, CVE_SW, CVE_Ports = fetch_keywords(str(CVE_Desc)) # ** Enable this after testing\n",
    "\n",
    "                    try: # Field: CVE-Language\n",
    "                        CVE_Lang = data['CVE_Items'][i]['cve']['problemtype']['problemtype_data'][0]['description'][0]['lang'] #CVE Language\n",
    "                    except IndexError: # Some CVEs are missing Vuln Type and Lang values.\n",
    "                        CVE_Lang = \"N/A\"\n",
    "                        M_CVE_Lang+=1\n",
    "\n",
    "                    try: # Field: CWE-ID\n",
    "                        CWE_ID = data['CVE_Items'][i]['cve']['problemtype']['problemtype_data'][0]['description'][0]['value'] #CVE Type\n",
    "                        # Counting CWEs with values NVD-CWE-Other and NVD-CWE-noinfo\n",
    "                        if CWE_ID == 'NVD-CWE-Other':\n",
    "                            M_CWE_ID_Other+=1\n",
    "                            CWE_Desc, CWE_Plat, CWE_Af_Res = \"N/A\", \"N/A\", \"N/A\"\n",
    "                            CWE_Consequences, CWE_Mitigations = {}, {}\n",
    "                            M_CWE_Desc, M_CWE_Plat, M_CWE_Af_Res, M_CWE_Consequences, M_CWE_Mitigations = (M_CWE_Desc+1), (M_CWE_Plat+1), (M_CWE_Af_Res+1), (M_CWE_Consequences+1), (M_CWE_Mitigations+1) # increment counters \n",
    "                        elif CWE_ID == 'NVD-CWE-noinfo':\n",
    "                            M_CWE_ID_Other2+=1\n",
    "                            CWE_Desc, CWE_Plat, CWE_Af_Res = \"N/A\", \"N/A\", \"N/A\"\n",
    "                            CWE_Consequences, CWE_Mitigations = {}, {}\n",
    "                            M_CWE_Desc, M_CWE_Plat, M_CWE_Af_Res, M_CWE_Consequences, M_CWE_Mitigations = (M_CWE_Desc+1), (M_CWE_Plat+1), (M_CWE_Af_Res+1), (M_CWE_Consequences+1), (M_CWE_Mitigations+1) # increment counters \n",
    "\n",
    "                        else: # when CWE-ID is an actual number\n",
    "                            CWE_Desc, CWE_Plat, CWE_Af_Res, CWE_Consequences, CWE_Mitigations = fetch_cwe_data(CWE_ID) # fetch CWE Data from CWE File.\n",
    "                            if CWE_Desc=='-': CWE_Desc=\"N/A\"\n",
    "                            if CWE_Plat=='-': CWE_Plat=\"N/A\"\n",
    "                            if CWE_Af_Res=='-': CWE_Af_Res=\"N/A\"\n",
    "                            \n",
    "                    except IndexError: # Some CVEs are missing Vuln Type and Lang values.\n",
    "                        # CWE_ID = \"N/A\" # this is not required. \n",
    "                        M_CWE_ID+=1\n",
    "\n",
    "                    try: # Field: severity\n",
    "                        CVE_Sev = data['CVE_Items'][i]['impact']['baseMetricV2']['severity']\n",
    "                    except KeyError: # Reserved CVEs will have this field value missing\n",
    "                        CVE_Sev = \"N/A\"\n",
    "                        M_CVE_Sev+=1\n",
    "\n",
    "                    try: # Field: V2 Score\n",
    "                        CVSS_V2 = data['CVE_Items'][i]['impact']['baseMetricV2']['cvssV2']['baseScore']\n",
    "                    except KeyError: # Reserved CVEs will have this field value missing\n",
    "                        CVSS_V2 = -1\n",
    "                        M_CVSS_V2+=1\n",
    "                        \n",
    "                    try: # Field: CVSS Score V3, not all CVEs have CVSS v3 Scores so we keep rest as zero.\n",
    "                        CVSS_V3 = data['CVE_Items'][i]['impact']['baseMetricV3']['cvssV3']['baseScore']\n",
    "                    except KeyError:\n",
    "                        CVSS_V3 = -1\n",
    "                        M_CVSS_V3+=1\n",
    "\n",
    "                    try: # Field: accessVector, not all CVEs have CVSS v3 Scores so we keep rest as zero.\n",
    "                        CVE_VAV = data['CVE_Items'][i]['impact']['baseMetricV2']['cvssV2']['accessVector']\n",
    "                    except KeyError:\n",
    "                        CVE_VAV = \"N/A\"\n",
    "                        M_CVE_VAV+=1\n",
    "\n",
    "                    try: # Field: userInteractionRequired, not all CVEs have CVSS v3 Scores so we keep rest as zero.\n",
    "                        CVE_UIR = data['CVE_Items'][i]['impact']['baseMetricV2']['userInteractionRequired']\n",
    "                    except KeyError:\n",
    "                        CVE_UIR = \"N/A\"\n",
    "                        M_CVE_UIR+=1    \n",
    "\n",
    "                    try: # Field: publishedDate, not all CVEs have CVSS v3 Scores so we keep rest as zero.\n",
    "                        CVE_PD = data['CVE_Items'][i]['publishedDate']\n",
    "                    except KeyError:\n",
    "                        CVE_PD = \"N/A\"\n",
    "                        M_CVE_PD+=1 \n",
    "\n",
    "                    try: # Field: lastModifiedDate, not all CVEs have CVSS v3 Scores so we keep rest as zero.\n",
    "                        CVE_MD = data['CVE_Items'][i]['lastModifiedDate']\n",
    "                    except KeyError:\n",
    "                        CVE_MD = \"N/A\"\n",
    "                        M_CVE_MD+=1 \n",
    "                    \n",
    "                    try:\n",
    "                        # Write row to the output CVS file...\n",
    "                        # enable next line for debugging...\n",
    "                        # print(CVE_ID, end=' ')\n",
    "                        try:\n",
    "                            writer.writerow([CVE_ID, CVE_Lang, CWE_ID, CWE_Desc, CWE_Plat, CWE_Af_Res, json.dumps(CWE_Consequences), json.dumps(CWE_Mitigations), CVE_Sev, CVSS_V2, \n",
    "                                CVSS_V3, CVE_VAV, CVE_UIR, json.dumps(CVE_OS), json.dumps(CVE_SW), json.dumps(CVE_Ports), \n",
    "                                CVE_PD, CVE_MD, CVE_Desc, url_tag_dict])\n",
    "                            # create DB Doc\n",
    "                            doc = {\"_id\": CVE_ID, \"lang\": CVE_Lang, \"cwe_id\": CWE_ID,\"cwe_desc\": CWE_Desc, \n",
    "                                   \"cwe_plat\": CWE_Plat, \"cwe_af_res\": CWE_Af_Res, \"cwe_consequences\": CWE_Consequences,\n",
    "                                   \"cwe_mitigations\": CWE_Mitigations, \"severity\": CVE_Sev, \"cvss_v2\": CVSS_V2, \n",
    "                                   \"cvss_v3\": CVSS_V3, \"vul_access_vector\": CVE_VAV,  \"user_int_req\": CVE_UIR, \"os\": CVE_OS, \n",
    "                                   \"sw\": CVE_SW,\"ports\": CVE_Ports, \"published_date\": CVE_PD, \"modified_date\": CVE_MD, \"description\": CVE_Desc, \n",
    "                                   \"url_and_tags\": url_tag_dict # json.dumps(url_tag_dict) \n",
    "                            } # print('Doc:\\n', json.dumps(doc, indent=4))\n",
    "                            \n",
    "                            # Insert or update the document in the database\n",
    "                            func.db.put(doc)\n",
    "\n",
    "                        except UnicodeEncodeError: # may occure for description field\n",
    "                            writer.writerow([CVE_ID, CVE_Lang, CWE_ID, CWE_Desc, CWE_Plat, CWE_Af_Res, json.dumps(CWE_Consequences), json.dumps(CWE_Mitigations), CVE_Sev, CVSS_V2, \n",
    "                                CVSS_V3, CVE_VAV, CVE_UIR, json.dumps(CVE_OS), json.dumps(CVE_SW), json.dumps(CVE_Ports), \n",
    "                                CVE_PD, CVE_MD, CVE_Desc.encode(\"utf-8\"), url_tag_dict])\n",
    "                            unicode_error+=1\n",
    "                            # create DB Doc\n",
    "                            doc = {\"_id\": CVE_ID, \"lang\": CVE_Lang, \"cwe_id\": CWE_ID, \"cwe_desc\": CWE_Desc, \n",
    "                                   \"cwe_plat\": CWE_Plat, \"cwe_af_res\": CWE_Af_Res, \"cwe_consequences\": CWE_Consequences,\n",
    "                                   \"cwe_mitigations\": CWE_Mitigations, \"severity\": CVE_Sev, \"cvss_v2\": CVSS_V2, \n",
    "                                   \"cvss_v3\": CVSS_V3, \"vul_access_vector\": CVE_VAV, \"user_int_req\": CVE_UIR, \"os\": CVE_OS, \n",
    "                                   \"sw\": CVE_SW, \"ports\": CVE_Ports, \"published_date\": CVE_PD, \"modified_date\": CVE_MD, \n",
    "                                   \"description\": CVE_Desc.encode(\"utf-8\"), \"url_and_tags\": url_tag_dict\n",
    "                            } # print('Doc:\\n', json.dumps(doc, indent=4))\n",
    "                            print('Doc:\\n', json.dumps(doc, indent=4))\n",
    "                            # Insert or update the document in the database\n",
    "                            func.db.put(doc)\n",
    "                    except TypeError:\n",
    "                        print('Type error with doc', CVE_ID)\n",
    "\n",
    "    print('Insertion complete.')\n",
    "    print('\\nTotal number of vulnerability records found:',vul_info_counter, '\\nVulnerabilities with missing information:')\n",
    "    print('Lang:\\t\\t'+str(M_CVE_Lang)+'\\n\\nCWE-ID:\\t\\t'+str(M_CWE_ID)+'\\nNVD-CWE-Other\\t'+str(M_CWE_ID_Other)+\n",
    "          '\\nNVD-CWE-noinfo:\\t'+str(M_CWE_ID_Other2)+'\\nTotal CWE:\\t'+str(M_CWE_ID+M_CWE_ID_Other+M_CWE_ID_Other2)+\n",
    "          '\\nMitigation:\\t'+str(M_CWE_Mitigations)+'\\nConsequences:\\t'+str(M_CWE_Consequences)+\n",
    "          '\\n\\nSeverity:\\t'+str(M_CVE_Sev)+'\\nCVSS_V2:\\t'+str(M_CVSS_V2)+\n",
    "          '\\nCVSS_V3:\\t'+str(M_CVSS_V3)+'\\nVul_access:\\t'+str(M_CVE_VAV)+'\\nUserIntReq:\\t'+str(M_CVE_UIR)+'\\nPublishDate:\\t'+\n",
    "          str(M_CVE_PD)+'\\nModifiedDate:\\t'+str(M_CVE_MD)+'\\nUnicodeError:\\t'+str(unicode_error))\n",
    "    print('\\nCWE_Desc:\\t'+str(M_CWE_Desc)+'\\nCWE_Plat:\\t'+str(M_CWE_Plat)+'\\nCWE_Af_Res:\\t'+str(M_CWE_Af_Res)+\n",
    "          '\\nM_OS:\\t\\t'+str(M_OS)+'\\nM_SW:\\t\\t'+str(M_SW)+'\\nM_Ports:\\t'+str(M_Ports))\n",
    "    print('\\n')\n",
    "\n",
    "# **************************************************************\n",
    "# 3. Fetch OS, SW Versions, and Port Numbers from given CVE Description...\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def fetch_keywords(text):\n",
    "    # extract port numbers from text\n",
    "    port_list = find_ports_from_text(text)\n",
    "    \n",
    "    # extract sw and os related terms.\n",
    "    match_list = []\n",
    "    patterns = [\n",
    "                [{'POS': 'PROPN'}, {'POS': {\"IN\": [\"PROPN\",\"NUM\", \"X\", \"VERB\"]}}],\n",
    "                [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': {\"IN\": [\"NUM\",\"X\",\"VERB\"]}}],    \n",
    "               ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"PROPN-PROPN-NUM\", None, patterns[0])\n",
    "    matcher.add(\"PROPN-PROPN-VERB\", None, patterns[1])\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        span = doc[start:end]\n",
    "        match_list.append(span.text)\n",
    "    \n",
    "    # Remove duplicates from the list\n",
    "    match_list = list(dict.fromkeys(match_list))\n",
    "    \n",
    "    # Remove subsets from the list ['Microsoft Windows 2000', 'Windows 2000']\n",
    "    for m in match_list:\n",
    "        for n in match_list:\n",
    "            if (len(m) > len(n)) and (set(n).issubset(set(m))):\n",
    "                match_list.remove(n)\n",
    "\n",
    "    # Extract OS from the list...\n",
    "    #ToDo: create an OS List and reference that list instead of these if statements.\n",
    "    os_list = [idx for idx in match_list if \n",
    "            idx.lower().startswith('Microsoft'.lower()) or idx.lower().startswith('Windows'.lower()) or \n",
    "            idx.lower().startswith('Ubuntu'.lower()) or idx.lower().startswith('Linux'.lower()) or \n",
    "            idx.lower().startswith('Apple'.lower()) or idx.lower().startswith('macOS'.lower()) or \n",
    "            idx.lower().startswith('RedHat'.lower()) or idx.lower().startswith('Red Hat'.lower()) or \n",
    "            idx.lower().startswith('CentOS'.lower()) or idx.lower().startswith('Fedora'.lower()) or \n",
    "            idx.lower().startswith('openSUSE'.lower())\n",
    "          ] \n",
    "\n",
    "    # Extract SW names from the list...\n",
    "    sw_list = []\n",
    "    for elem in match_list:\n",
    "        if elem not in os_list:\n",
    "            sw_list.append(elem) \n",
    "    return os_list, sw_list, port_list\n",
    "\n",
    "\n",
    "# Port extractor from string... \n",
    "def find_ports_from_text(text):\n",
    "    port_list = []\n",
    "    nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "    doc = nlp(text)\n",
    "    # ? (0/1) + (1+) * (0+)\n",
    "    port_pattern = re.compile(r\"[Pp](ort)[s]? [:]?(\\d+)?((\\d+|less than | and |, |,|/|/ | / | |-)?(\\d+))*\") #  \n",
    "\n",
    "    for match in re.finditer(port_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        # if text is only 'port ' then discard it\n",
    "        if doc.text[start:end].lower() != 'port ' or doc.text[start:end].lower() != 'ports ': \n",
    "            port_list.append(doc.text[start:end])\n",
    "        span = doc.char_span(start, end)\n",
    "    # print('\\nPorts related keywords:\\n', port_list)\n",
    "    return port_list\n",
    "\n",
    "\n",
    "# Referencing CWE File for fetching CWE Descriptions and related data...\n",
    "# Extracting 'CWE_Desc', 'CWE_Plat', 'CWE_Af_Res'\n",
    "def fetch_cwe_data(given_cwe_id):  \n",
    "    result = cwe_data.loc[cwe_data['_id'] == given_cwe_id] #_id (CWE-ID) on cwe_combined file\n",
    "    CWE_Description = result.iloc[0,1]\n",
    "    CWE_App_Plat = result.iloc[0,8]\n",
    "    CWE_Aff_Res = result.iloc[0,19]\n",
    "    CWE_Consequences = process_consequences(result.iloc[0,14]) # process received data and make a dictionary\n",
    "    CWE_Mitigations = process_mitigations(result.iloc[0,16]) # process received data and make a dictionary\n",
    "    return CWE_Description, CWE_App_Plat, CWE_Aff_Res, CWE_Consequences, CWE_Mitigations\n",
    "\n",
    "\n",
    "# process consequences text and generate a dictionary out of it.\n",
    "def process_consequences(con):\n",
    "    data = {}\n",
    "    scope_list = con.split('::SCOPE:') # Split by ::SCORE:\n",
    "    scope_list = [x for x in scope_list if x] # remove blank items ('')\n",
    "    scope_list = [x.replace('::', '') for x in scope_list] # remove :: from end of items\n",
    "\n",
    "    for i in scope_list: # loop through scopes  \n",
    "        sub_data={}\n",
    "        kw = i.split(':')[0] # first word of sentence ''\n",
    "        j = i.replace(kw, '') # remove keyword from the item\n",
    "        \n",
    "        scope_sub_items = re.split(r'(:SCOPE:|:IMPACT:|:NOTE:)',j) \n",
    "        scope_sub_items = [x for x in scope_sub_items if x] # remove blank items ('')\n",
    "        for item in range(0, len(scope_sub_items), 2): # concat following items 1*2, 3*4, etc.\n",
    "            new_form = scope_sub_items[item].replace(':', '')+'*'+scope_sub_items[item+1]\n",
    "            c = new_form.split('*')\n",
    "            if c[0] not in sub_data:\n",
    "                sub_data[c[0]] = []\n",
    "            sub_data[c[0]].append(c[1])\n",
    "        \n",
    "        if kw not in data:\n",
    "            data[kw]=sub_data\n",
    "    return data\n",
    " \n",
    "\n",
    "# process mitigation text and generate a dictionary out of it.\n",
    "def process_mitigations(mit):\n",
    "    data = {}\n",
    "    scope_list = mit.split('::PHASE:') # Split by ::SCORE:\n",
    "    scope_list = [x for x in scope_list if x] # remove blank items ('')\n",
    "    scope_list = [x.replace('::', '') for x in scope_list] # remove :: from end of items\n",
    "\n",
    "    for i in scope_list: # loop through scopes  \n",
    "        sub_data={}\n",
    "        kw = i.split(':')[0] # first word of sentence ''\n",
    "        j = i.replace(kw, '') # remove keyword from the item\n",
    "        \n",
    "        scope_sub_items = re.split(r'(:STRATEGY:|:DESCRIPTION:|:EFFECTIVENESS:)',j) \n",
    "        scope_sub_items = [x for x in scope_sub_items if x] # remove blank items ('')\n",
    "        for item in range(0, len(scope_sub_items), 2): # concat following items 1*2, 3*4, etc.\n",
    "            new_form = scope_sub_items[item].replace(':', '')+'*'+scope_sub_items[item+1]\n",
    "            c = new_form.split('*')\n",
    "            if c[0] not in sub_data:\n",
    "                sub_data[c[0]] = []\n",
    "\n",
    "            sub_data[c[0]].append(c[1])\n",
    "\n",
    "        if kw not in data: # if key not present in data\n",
    "            data[kw]=[]\n",
    "            data[kw].append(sub_data)\n",
    "\n",
    "        else: # if key present\n",
    "            for k, v in sub_data.items(): # add under same keys\n",
    "                if k in data[kw][0]: # if key found\n",
    "                    if v not in data[kw][0][k]: # if value not found, add it under value list\n",
    "                        data[kw][0][k].extend(v)\n",
    "                else: \n",
    "                    data[kw][0][k] = v\n",
    "\n",
    "    d = {k:v[0] for k,v in data.items()} # structure fix, list to dict.\n",
    "    return d\n",
    "\n",
    "print('*** CyVID Dataset Preparation v5 ***\\n')\n",
    "# time the execution\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start Time:\", current_time)\n",
    "startTime = datetime.now() # start timer\n",
    "spinner.start()\n",
    "\n",
    "# Step 1: Check and remove existing files and folders\n",
    "func.check_files(analysis_path)\n",
    "\n",
    "# Step 2, read JSON files, summarize data and extract keywords...\n",
    "read_and_summarize()\n",
    "# Step 3 and 4 will execute based on current CWE-ID and CVE-Description, therefore it will run within Step 2.\n",
    "\n",
    "print('\\nData collected.\\nFile(s) saved at '+analysis_path+'/. and Database: '+db_name+' created.')\n",
    "print(\"Execution time: \"+str(datetime.now() - startTime))\n",
    "spinner.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As of 7/21/21 166759 CVEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
